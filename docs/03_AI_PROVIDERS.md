# ü§ñ AI –ü–†–û–í–ê–ô–î–ï–†–´

> –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ AI –º–æ–¥–µ–ª–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ñ–æ—Ç–æ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¢–ó

---

## üìä –û–ë–ó–û–† –ü–†–û–í–ê–ô–î–ï–†–û–í

| –ü—Ä–æ–≤–∞–π–¥–µ—Ä | –ú–æ–¥–µ–ª—å | –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ | –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç | –õ–∏–º–∏—Ç—ã |
|-----------|--------|------------|-----------|--------|
| **Z.AI GLM** | glm-4-6v / glm-4-plus | Vision + Text | Primary | –ü–æ –ø–æ–¥–ø–∏—Å–∫–µ |
| **Z.AI GLM Flash** | glm-4v-flash / glm-4-flash | Vision + Text (–±—ã—Å—Ç—Ä—ã–µ) | Primary | –ü–æ –ø–æ–¥–ø–∏—Å–∫–µ |
| **Google Gemini** | gemini-1.5-flash | Vision + Text | Fallback | 60 RPM –±–µ—Å–ø–ª–∞—Ç–Ω–æ |

---

## üîë API –ö–õ–Æ–ß–ò

### Z.AI (GLM) - –û–§–ò–¶–ò–ê–õ–¨–ù–ê–Ø –ë–ò–ë–õ–ò–û–¢–ï–ö–ê

**–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏:**
```bash
pip install zhipuai>=2.1.5.20250131
```

- **–ü–æ–ª—É—á–∏—Ç—å –∫–ª—é—á:** https://z.ai/manage-apikey/apikey-list
- **–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è:** https://open.bigmodel.cn/dev/api
- **–ú–æ–¥–µ–ª–∏:**
  - `glm-4-6v` ‚Äî Vision GLM-4.6V (–ª—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)
  - `glm-4v-flash` ‚Äî Vision Fast (–±—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)
  - `glm-4-plus` ‚Äî Text (–ª—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞)
  - `glm-4-air` ‚Äî Text (—Å—Ä–µ–¥–Ω–∏–π –±–∞–ª–∞–Ω—Å)
  - `glm-4-flash` ‚Äî Text Fast (–±—ã—Å—Ç—Ä–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è)

**–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ zhipuai:**
- ‚úÖ –ü–æ–ª–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π GLM
- ‚úÖ –°–æ–≤–º–µ—Å—Ç–∏–º–∞ —Å aiogram (—á–µ—Ä–µ–∑ async/await)
- ‚úÖ –í—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫
- ‚úÖ –û—Ñ–∏—Ü–∏–∞–ª—å–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –æ—Ç Z.ai

### Google Gemini
- **–ü–æ–ª—É—á–∏—Ç—å:** https://aistudio.google.com/apikey
- **–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è:** https://ai.google.dev/docs
- **–ú–æ–¥–µ–ª–∏:**
  - `gemini-1.5-flash` ‚Äî –ë—ã—Å—Ç—Ä—ã–π, –±–µ—Å–ø–ª–∞—Ç–Ω—ã–π, Vision + Text
  - `gemini-1.5-pro` ‚Äî –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–µ–µ, –º–µ–Ω—å—à–µ –ª–∏–º–∏—Ç—ã

---

## üèó –ê–†–•–ò–¢–ï–ö–¢–£–†–ê

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      AIProviderChain                            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îÇ
‚îÇ  ‚îÇ  VisionChain    ‚îÇ    ‚îÇ   TextChain     ‚îÇ                    ‚îÇ
‚îÇ  ‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ                    ‚îÇ
‚îÇ  ‚îÇ  1. GLM-4V ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ    ‚îÇ  1. GLM-4 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ                    ‚îÇ
‚îÇ  ‚îÇ       ‚îÇ         ‚îÇ    ‚îÇ       ‚îÇ         ‚îÇ                    ‚îÇ
‚îÇ  ‚îÇ       ‚ñº fail    ‚îÇ    ‚îÇ       ‚ñº fail    ‚îÇ                    ‚îÇ
‚îÇ  ‚îÇ  2. Gemini ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ    ‚îÇ  2. Gemini ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ                    ‚îÇ
‚îÇ  ‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ                    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìù –ë–ê–ó–û–í–´–ï –ö–õ–ê–°–°–´

```python
# core/ai_providers/base.py

from abc import ABC, abstractmethod
from typing import Optional, List
from dataclasses import dataclass
from enum import Enum


class ProviderStatus(Enum):
    """–°—Ç–∞—Ç—É—Å –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞."""
    AVAILABLE = "available"
    RATE_LIMITED = "rate_limited"
    ERROR = "error"
    DISABLED = "disabled"


@dataclass
class ProviderResponse:
    """–û—Ç–≤–µ—Ç –æ—Ç AI –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞."""
    success: bool
    content: str
    provider_name: str
    tokens_used: Optional[int] = None
    error_message: Optional[str] = None


class BaseVisionProvider(ABC):
    """–ê–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è Vision –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤."""
    
    name: str = "base_vision"
    
    @abstractmethod
    async def analyze_image(
        self, 
        image_bytes: bytes,
        prompt: Optional[str] = None
    ) -> ProviderResponse:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ.
        
        Args:
            image_bytes: –ë–∞–π—Ç—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (JPEG/PNG)
            prompt: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            ProviderResponse —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –∞–Ω–∞–ª–∏–∑–∞
        """
        pass
    
    @abstractmethod
    async def analyze_multiple_images(
        self,
        images: List[bytes],
        prompt: Optional[str] = None
    ) -> ProviderResponse:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤–º–µ—Å—Ç–µ.
        
        Args:
            images: –°–ø–∏—Å–æ–∫ –±–∞–π—Ç–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            prompt: –û–±—â–∏–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            ProviderResponse —Å –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º
        """
        pass
    
    async def health_check(self) -> ProviderStatus:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞."""
        try:
            # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            test_image = self._create_test_image()
            response = await self.analyze_image(test_image, "Test")
            return ProviderStatus.AVAILABLE if response.success else ProviderStatus.ERROR
        except Exception:
            return ProviderStatus.ERROR
    
    def _create_test_image(self) -> bytes:
        """–°–æ–∑–¥–∞—ë—Ç –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ."""
        # 1x1 –±–µ–ª—ã–π –ø–∏–∫—Å–µ–ª—å PNG
        return bytes([
            0x89, 0x50, 0x4E, 0x47, 0x0D, 0x0A, 0x1A, 0x0A,
            0x00, 0x00, 0x00, 0x0D, 0x49, 0x48, 0x44, 0x52,
            0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x01,
            0x08, 0x02, 0x00, 0x00, 0x00, 0x90, 0x77, 0x53,
            0xDE, 0x00, 0x00, 0x00, 0x0C, 0x49, 0x44, 0x41,
            0x54, 0x08, 0xD7, 0x63, 0xF8, 0xFF, 0xFF, 0x3F,
            0x00, 0x05, 0xFE, 0x02, 0xFE, 0xDC, 0xCC, 0x59,
            0xE7, 0x00, 0x00, 0x00, 0x00, 0x49, 0x45, 0x4E,
            0x44, 0xAE, 0x42, 0x60, 0x82
        ])


class BaseTextProvider(ABC):
    """–ê–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è Text –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤."""
    
    name: str = "base_text"
    
    @abstractmethod
    async def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        max_tokens: int = 4000,
        temperature: float = 0.7
    ) -> ProviderResponse:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –ø–æ –ø—Ä–æ–º–ø—Ç—É.
        
        Args:
            prompt: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ø—Ä–æ–º–ø—Ç
            system_prompt: –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç (—Ä–æ–ª—å AI)
            max_tokens: –ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ
            temperature: –ö—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å (0.0-1.0)
            
        Returns:
            ProviderResponse —Å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º
        """
        pass
    
    async def health_check(self) -> ProviderStatus:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞."""
        try:
            response = await self.generate("Say 'OK'", max_tokens=10)
            return ProviderStatus.AVAILABLE if response.success else ProviderStatus.ERROR
        except Exception:
            return ProviderStatus.ERROR
```

---

## üî∑ Z.AI GLM –ü–†–û–í–ê–ô–î–ï–† (–û–§–ò–¶–ò–ê–õ–¨–ù–ê–Ø –ë–ò–ë–õ–ò–û–¢–ï–ö–ê)

```python
# core/ai_providers/glm.py

import asyncio
import base64
import structlog
from typing import Optional, List
from zhipuai import ZhipuAI

from core.ai_providers.base import (
    BaseVisionProvider, 
    BaseTextProvider, 
    ProviderResponse
)

logger = structlog.get_logger()


class GLMProvider(BaseVisionProvider, BaseTextProvider):
    """
    –ü—Ä–æ–≤–∞–π–¥–µ—Ä Z.AI GLM-4 / GLM-4V.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –û–§–ò–¶–ò–ê–õ–¨–ù–£–Æ –±–∏–±–ª–∏–æ—Ç–µ–∫—É zhipuai>=2.1.5.20250131
    –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: https://open.bigmodel.cn/dev/api
    """
    
    name = "glm"
    
    # –ú–æ–¥–µ–ª–∏
    VISION_MODEL = "glm-4-6v"      # GLM-4.6V Vision (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
    VISION_FAST = "glm-4v-flash"   # –ë—ã—Å—Ç—Ä—ã–π Vision
    TEXT_MODEL = "glm-4-plus"      # GLM-4 Plus (–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π)
    TEXT_FAST = "glm-4-flash"      # –ë—ã—Å—Ç—Ä—ã–π Text
    
    def __init__(
        self, 
        api_key: str,
        use_fast_models: bool = False
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞.
        
        Args:
            api_key: API –∫–ª—é—á Z.AI
            use_fast_models: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å flash –º–æ–¥–µ–ª–∏ (–±—ã—Å—Ç—Ä–µ–µ, –¥–µ—à–µ–≤–ª–µ)
        """
        self.api_key = api_key
        self.use_fast_models = use_fast_models
        self._client = ZhipuAI(api_key=api_key)
        
        # –í—ã–±–æ—Ä –º–æ–¥–µ–ª–µ–π
        self.vision_model = self.VISION_FAST if use_fast_models else self.VISION_MODEL
        self.text_model = self.TEXT_FAST if use_fast_models else self.TEXT_MODEL
    
    async def analyze_image(
        self, 
        image_bytes: bytes,
        prompt: Optional[str] = None
    ) -> ProviderResponse:
        """–ê–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è."""
        return await self.analyze_multiple_images([image_bytes], prompt)
    
    async def analyze_multiple_images(
        self,
        images: List[bytes],
        prompt: Optional[str] = None
    ) -> ProviderResponse:
        """
        –ê–Ω–∞–ª–∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.
        
        GLM-4.6V –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –æ–¥–Ω–æ–º –∑–∞–ø—Ä–æ—Å–µ.
        """
        if not prompt:
            prompt = "–û–ø–∏—à–∏ –ø–æ–¥—Ä–æ–±–Ω–æ —á—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–æ –Ω–∞ —Ñ–æ—Ç–æ."
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º content —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ (base64)
        content = []
        for img_bytes in images[:5]:  # –ú–∞–∫—Å–∏–º—É–º 5 —Ñ–æ—Ç–æ
            base64_image = base64.b64encode(img_bytes).decode("utf-8")
            content.append({
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/jpeg;base64,{base64_image}"
                }
            })
        
        content.append({
            "type": "text",
            "text": prompt
        })
        
        try:
            # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤—ã–∑–æ–≤ —á–µ—Ä–µ–∑ executor
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: self._client.chat.completions.create(
                    model=self.vision_model,
                    messages=[
                        {
                            "role": "user",
                            "content": content
                        }
                    ],
                    temperature=0.3,
                    max_tokens=2000
                )
            )
            
            result_text = response.choices[0].message.content
            tokens_used = getattr(response.usage, 'total_tokens', None)
            
            logger.info(
                "glm_vision_success",
                model=self.vision_model,
                images_count=len(images),
                tokens=tokens_used
            )
            
            return ProviderResponse(
                success=True,
                content=result_text,
                provider_name=self.name,
                tokens_used=tokens_used
            )
            
        except Exception as e:
            error_msg = str(e)
            logger.error("glm_vision_error", error=error_msg)
            return ProviderResponse(
                success=False,
                content="",
                provider_name=self.name,
                error_message=error_msg
            )
    
    async def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        max_tokens: int = 4000,
        temperature: float = 0.7
    ) -> ProviderResponse:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞."""
        
        messages = []
        
        if system_prompt:
            messages.append({
                "role": "system",
                "content": system_prompt
            })
        
        messages.append({
            "role": "user",
            "content": prompt
        })
        
        try:
            # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤—ã–∑–æ–≤ —á–µ—Ä–µ–∑ executor
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: self._client.chat.completions.create(
                    model=self.text_model,
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens
                )
            )
            
            result_text = response.choices[0].message.content
            tokens_used = getattr(response.usage, 'total_tokens', None)
            
            logger.info(
                "glm_text_success",
                model=self.text_model,
                tokens=tokens_used,
                result_length=len(result_text)
            )
            
            return ProviderResponse(
                success=True,
                content=result_text,
                provider_name=self.name,
                tokens_used=tokens_used
            )
            
        except Exception as e:
            error_msg = str(e)
            logger.error("glm_text_error", error=error_msg)
            return ProviderResponse(
                success=False,
                content="",
                provider_name=self.name,
                error_message=error_msg
            )
    
    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç—å –∫–ª–∏–µ–Ω—Ç (–Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è –¥–ª—è zhipuai)."""
        pass
            "temperature": 0.7
        }
        
        try:
            # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤—ã–∑–æ–≤ —á–µ—Ä–µ–∑ executor
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: self._client.chat.completions.create(
                    model=self.vision_model,
                    messages=[
                        {
                            "role": "user",
                            "content": content
                        }
                    ],
                    temperature=0.3,
                    max_tokens=2000
                )
            )
            
            result_text = response.choices[0].message.content
            tokens_used = getattr(response.usage, 'total_tokens', None)
            
            logger.info(
                "glm_vision_success",
                model=self.vision_model,
                images_count=len(images),
                tokens=tokens_used
            )
            
            return ProviderResponse(
                success=True,
                content=result_text,
                provider_name=self.name,
                tokens_used=tokens_used
            )
            
        except Exception as e:
            error_msg = str(e)
            logger.error("glm_vision_error", error=error_msg)
            return ProviderResponse(
                success=False,
                content="",
                provider_name=self.name,
                error_message=error_msg
            )
    
    async def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        max_tokens: int = 4000,
        temperature: float = 0.7
    ) -> ProviderResponse:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞."""
        
        messages = []
        
        if system_prompt:
            messages.append({
                "role": "system",
                "content": system_prompt
            })
        
        messages.append({
            "role": "user",
            "content": prompt
        })
        
        try:
            # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤—ã–∑–æ–≤ —á–µ—Ä–µ–∑ executor
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: self._client.chat.completions.create(
                    model=self.text_model,
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens
                )
            )
            
            result_text = response.choices[0].message.content
            tokens_used = getattr(response.usage, 'total_tokens', None)
            
            logger.info(
                "glm_text_success",
                model=self.text_model,
                tokens=tokens_used,
                result_length=len(result_text)
            )
            
            return ProviderResponse(
                success=True,
                content=result_text,
                provider_name=self.name,
                tokens_used=tokens_used
            )
            
        except Exception as e:
            error_msg = str(e)
            logger.error("glm_text_error", error=error_msg)
            return ProviderResponse(
                success=False,
                content="",
                provider_name=self.name,
                error_message=error_msg
            )
    
    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç—å –∫–ª–∏–µ–Ω—Ç (–Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è –¥–ª—è zhipuai)."""
        pass
```

---

## üî∂ GOOGLE GEMINI –ü–†–û–í–ê–ô–î–ï–† (FALLBACK)

```python
# core/ai_providers/gemini.py

import base64
import httpx
import structlog
from typing import Optional, List

from core.ai_providers.base import (
    BaseVisionProvider, 
    BaseTextProvider, 
    ProviderResponse
)

logger = structlog.get_logger()


class GeminiProvider(BaseVisionProvider, BaseTextProvider):
    """
    –ü—Ä–æ–≤–∞–π–¥–µ—Ä Google Gemini.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞–∫ fallback –∫–æ–≥–¥–∞ GLM –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω.
    –ë–µ—Å–ø–ª–∞—Ç–Ω—ã–π tier: 60 RPM, 1M tokens/day.
    
    –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: https://ai.google.dev/docs
    """
    
    name = "gemini"
    BASE_URL = "https://generativelanguage.googleapis.com/v1beta"
    MODEL = "gemini-1.5-flash"
    
    def __init__(
        self, 
        api_key: str, 
        timeout: float = 60.0
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞.
        
        Args:
            api_key: API –∫–ª—é—á Google AI Studio
            timeout: –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        """
        self.api_key = api_key
        self.timeout = timeout
        self._client: Optional[httpx.AsyncClient] = None
    
    async def _get_client(self) -> httpx.AsyncClient:
        """Lazy –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è HTTP –∫–ª–∏–µ–Ω—Ç–∞."""
        if self._client is None or self._client.is_closed:
            self._client = httpx.AsyncClient(
                base_url=self.BASE_URL,
                timeout=self.timeout
            )
        return self._client
    
    async def analyze_image(
        self, 
        image_bytes: bytes,
        prompt: Optional[str] = None
    ) -> ProviderResponse:
        """–ê–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è."""
        return await self.analyze_multiple_images([image_bytes], prompt)
    
    async def analyze_multiple_images(
        self,
        images: List[bytes],
        prompt: Optional[str] = None
    ) -> ProviderResponse:
        """
        –ê–Ω–∞–ª–∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.
        
        Gemini –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.
        """
        if not prompt:
            prompt = "–û–ø–∏—à–∏ –ø–æ–¥—Ä–æ–±–Ω–æ —á—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–æ –Ω–∞ —Ñ–æ—Ç–æ."
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º parts —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
        parts = []
        
        for img_bytes in images:
            base64_image = base64.b64encode(img_bytes).decode("utf-8")
            parts.append({
                "inline_data": {
                    "mime_type": "image/jpeg",
                    "data": base64_image
                }
            })
        
        parts.append({"text": prompt})
        
        payload = {
            "contents": [{
                "parts": parts
            }],
            "generationConfig": {
                "temperature": 0.7,
                "maxOutputTokens": 2000
            }
        }
        
        url = f"/models/{self.MODEL}:generateContent?key={self.api_key}"
        
        try:
            client = await self._get_client()
            response = await client.post(url, json=payload)
            response.raise_for_status()
            
            data = response.json()
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –æ—Ç–≤–µ—Ç–∞
            candidates = data.get("candidates", [])
            if not candidates:
                return ProviderResponse(
                    success=False,
                    content="",
                    provider_name=self.name,
                    error_message="No candidates in response"
                )
            
            content = candidates[0].get("content", {})
            parts = content.get("parts", [])
            result_text = "".join(p.get("text", "") for p in parts)
            
            # –¢–æ–∫–µ–Ω—ã –∏–∑ usageMetadata
            usage = data.get("usageMetadata", {})
            tokens_used = usage.get("totalTokenCount")
            
            logger.info(
                "gemini_vision_success",
                images_count=len(images),
                tokens=tokens_used
            )
            
            return ProviderResponse(
                success=True,
                content=result_text,
                provider_name=self.name,
                tokens_used=tokens_used
            )
            
        except httpx.HTTPStatusError as e:
            error_msg = f"HTTP {e.response.status_code}: {e.response.text}"
            logger.error("gemini_vision_http_error", error=error_msg)
            return ProviderResponse(
                success=False,
                content="",
                provider_name=self.name,
                error_message=error_msg
            )
            
        except Exception as e:
            logger.error("gemini_vision_error", error=str(e))
            return ProviderResponse(
                success=False,
                content="",
                provider_name=self.name,
                error_message=str(e)
            )
    
    async def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        max_tokens: int = 4000,
        temperature: float = 0.7
    ) -> ProviderResponse:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞."""
        
        # Gemini –∏—Å–ø–æ–ª—å–∑—É–µ—Ç systemInstruction –¥–ª—è —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
        payload = {
            "contents": [{
                "parts": [{"text": prompt}]
            }],
            "generationConfig": {
                "temperature": temperature,
                "maxOutputTokens": max_tokens
            }
        }
        
        if system_prompt:
            payload["systemInstruction"] = {
                "parts": [{"text": system_prompt}]
            }
        
        url = f"/models/{self.MODEL}:generateContent?key={self.api_key}"
        
        try:
            client = await self._get_client()
            response = await client.post(url, json=payload)
            response.raise_for_status()
            
            data = response.json()
            
            candidates = data.get("candidates", [])
            if not candidates:
                return ProviderResponse(
                    success=False,
                    content="",
                    provider_name=self.name,
                    error_message="No candidates in response"
                )
            
            content = candidates[0].get("content", {})
            parts = content.get("parts", [])
            result_text = "".join(p.get("text", "") for p in parts)
            
            usage = data.get("usageMetadata", {})
            tokens_used = usage.get("totalTokenCount")
            
            logger.info(
                "gemini_text_success",
                tokens=tokens_used,
                result_length=len(result_text)
            )
            
            return ProviderResponse(
                success=True,
                content=result_text,
                provider_name=self.name,
                tokens_used=tokens_used
            )
            
        except httpx.HTTPStatusError as e:
            error_msg = f"HTTP {e.response.status_code}: {e.response.text}"
            logger.error("gemini_text_http_error", error=error_msg)
            return ProviderResponse(
                success=False,
                content="",
                provider_name=self.name,
                error_message=error_msg
            )
            
        except Exception as e:
            logger.error("gemini_text_error", error=str(e))
            return ProviderResponse(
                success=False,
                content="",
                provider_name=self.name,
                error_message=str(e)
            )
    
    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç—å HTTP –∫–ª–∏–µ–Ω—Ç."""
        if self._client and not self._client.is_closed:
            await self._client.aclose()
            self._client = None
```

---

## üîó CHAIN –ü–†–û–í–ê–ô–î–ï–†–û–í (FALLBACK –õ–û–ì–ò–ö–ê)

```python
# core/ai_providers/chain.py

import structlog
from typing import List, Optional
from dataclasses import dataclass

from core.ai_providers.base import (
    BaseVisionProvider, 
    BaseTextProvider, 
    ProviderResponse,
    ProviderStatus
)

logger = structlog.get_logger()


@dataclass
class ChainConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è chain –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤."""
    max_retries: int = 2
    retry_delay: float = 1.0


class VisionProviderChain:
    """
    Chain Vision –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º fallback.
    
    –ï—Å–ª–∏ –ø–µ—Ä–≤—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω ‚Äî –ø—Ä–æ–±—É–µ—Ç —Å–ª–µ–¥—É—é—â–∏–π.
    """
    
    def __init__(
        self, 
        providers: List[BaseVisionProvider],
        config: Optional[ChainConfig] = None
    ):
        """
        Args:
            providers: –°–ø–∏—Å–æ–∫ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ –≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è chain
        """
        self.providers = providers
        self.config = config or ChainConfig()
        self._provider_status: dict[str, ProviderStatus] = {}
    
    async def analyze_image(
        self, 
        image_bytes: bytes,
        prompt: Optional[str] = None
    ) -> ProviderResponse:
        """–ê–Ω–∞–ª–∏–∑ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º fallback."""
        return await self.analyze_multiple_images([image_bytes], prompt)
    
    async def analyze_multiple_images(
        self,
        images: List[bytes],
        prompt: Optional[str] = None
    ) -> ProviderResponse:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –ø—Ä–æ–±—É—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ –ø–æ –æ—á–µ—Ä–µ–¥–∏.
        
        Returns:
            ProviderResponse –æ—Ç –ø–µ—Ä–≤–æ–≥–æ —É—Å–ø–µ—à–Ω–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
            
        Raises:
            RuntimeError: –ï—Å–ª–∏ –≤—Å–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã
        """
        last_error = None
        
        for provider in self.providers:
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä –ø–æ–º–µ—á–µ–Ω –∫–∞–∫ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã–π
            status = self._provider_status.get(provider.name)
            if status == ProviderStatus.DISABLED:
                continue
            
            logger.info(
                "trying_vision_provider",
                provider=provider.name,
                images_count=len(images)
            )
            
            for attempt in range(self.config.max_retries):
                response = await provider.analyze_multiple_images(images, prompt)
                
                if response.success and response.content:
                    logger.info(
                        "vision_provider_success",
                        provider=provider.name,
                        attempt=attempt + 1
                    )
                    return response
                
                last_error = response.error_message
                logger.warning(
                    "vision_provider_attempt_failed",
                    provider=provider.name,
                    attempt=attempt + 1,
                    error=last_error
                )
            
            # –ü—Ä–æ–≤–∞–π–¥–µ—Ä –Ω–µ –æ—Ç–≤–µ—Ç–∏–ª –ø–æ—Å–ª–µ –≤—Å–µ—Ö –ø–æ–ø—ã—Ç–æ–∫
            logger.error(
                "vision_provider_failed",
                provider=provider.name,
                error=last_error
            )
        
        # –í—Å–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã
        raise RuntimeError(
            f"–í—Å–µ Vision –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã. –ü–æ—Å–ª–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞: {last_error}"
        )
    
    async def health_check_all(self) -> dict[str, ProviderStatus]:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –≤—Å–µ—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤."""
        for provider in self.providers:
            status = await provider.health_check()
            self._provider_status[provider.name] = status
            logger.info(
                "provider_health_check",
                provider=provider.name,
                status=status.value
            )
        return self._provider_status


class TextProviderChain:
    """Chain Text –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º fallback."""
    
    def __init__(
        self, 
        providers: List[BaseTextProvider],
        config: Optional[ChainConfig] = None
    ):
        self.providers = providers
        self.config = config or ChainConfig()
        self._provider_status: dict[str, ProviderStatus] = {}
    
    async def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        max_tokens: int = 4000,
        temperature: float = 0.7
    ) -> ProviderResponse:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç, –ø—Ä–æ–±—É—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ –ø–æ –æ—á–µ—Ä–µ–¥–∏.
        """
        last_error = None
        
        for provider in self.providers:
            status = self._provider_status.get(provider.name)
            if status == ProviderStatus.DISABLED:
                continue
            
            logger.info("trying_text_provider", provider=provider.name)
            
            for attempt in range(self.config.max_retries):
                response = await provider.generate(
                    prompt=prompt,
                    system_prompt=system_prompt,
                    max_tokens=max_tokens,
                    temperature=temperature
                )
                
                if response.success and response.content:
                    logger.info(
                        "text_provider_success",
                        provider=provider.name,
                        attempt=attempt + 1,
                        length=len(response.content)
                    )
                    return response
                
                last_error = response.error_message
                logger.warning(
                    "text_provider_attempt_failed",
                    provider=provider.name,
                    attempt=attempt + 1,
                    error=last_error
                )
            
            logger.error(
                "text_provider_failed",
                provider=provider.name,
                error=last_error
            )
        
        raise RuntimeError(
            f"–í—Å–µ Text –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã. –ü–æ—Å–ª–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞: {last_error}"
        )
    
    async def health_check_all(self) -> dict[str, ProviderStatus]:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –≤—Å–µ—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤."""
        for provider in self.providers:
            status = await provider.health_check()
            self._provider_status[provider.name] = status
        return self._provider_status
```

---

## üè≠ –§–ê–ë–†–ò–ö–ê –ü–†–û–í–ê–ô–î–ï–†–û–í

```python
# core/ai_providers/__init__.py

from bot.config import settings
from core.ai_providers.glm import GLMProvider
from core.ai_providers.gemini import GeminiProvider
from core.ai_providers.chain import VisionProviderChain, TextProviderChain, ChainConfig


def create_vision_chain() -> VisionProviderChain:
    """
    –°–æ–∑–¥–∞—ë—Ç chain Vision –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤.
    
    –ü–æ—Ä—è–¥–æ–∫: GLM-4V ‚Üí Gemini Flash
    """
    providers = []
    
    # Primary: GLM-4V
    if settings.glm_api_key:
        providers.append(GLMProvider(settings.glm_api_key))
    
    # Fallback: Gemini
    if settings.gemini_api_key:
        providers.append(GeminiProvider(settings.gemini_api_key))
    
    if not providers:
        raise ValueError("–ù–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω –Ω–∏ –æ–¥–∏–Ω Vision –ø—Ä–æ–≤–∞–π–¥–µ—Ä!")
    
    return VisionProviderChain(providers, ChainConfig(max_retries=2))


def create_text_chain() -> TextProviderChain:
    """
    –°–æ–∑–¥–∞—ë—Ç chain Text –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤.
    
    –ü–æ—Ä—è–¥–æ–∫: GLM-4 ‚Üí Gemini Flash
    """
    providers = []
    
    # Primary: GLM-4
    if settings.glm_api_key:
        providers.append(GLMProvider(settings.glm_api_key))
    
    # Fallback: Gemini
    if settings.gemini_api_key:
        providers.append(GeminiProvider(settings.gemini_api_key))
    
    if not providers:
        raise ValueError("–ù–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω –Ω–∏ –æ–¥–∏–Ω Text –ø—Ä–æ–≤–∞–π–¥–µ—Ä!")
    
    return TextProviderChain(providers, ChainConfig(max_retries=2))
```

---

## üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ü–†–û–í–ê–ô–î–ï–†–û–í

```python
# tests/test_ai_providers.py

import pytest
from unittest.mock import AsyncMock, patch

from core.ai_providers.base import ProviderResponse
from core.ai_providers.glm import GLMProvider
from core.ai_providers.gemini import GeminiProvider
from core.ai_providers.chain import VisionProviderChain


@pytest.fixture
def mock_glm():
    """–ú–æ–∫ GLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞."""
    provider = AsyncMock(spec=GLMProvider)
    provider.name = "glm"
    return provider


@pytest.fixture
def mock_gemini():
    """–ú–æ–∫ Gemini –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞."""
    provider = AsyncMock(spec=GeminiProvider)
    provider.name = "gemini"
    return provider


@pytest.mark.asyncio
async def test_vision_chain_first_provider_success(mock_glm, mock_gemini):
    """–¢–µ—Å—Ç: –ø–µ—Ä–≤—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä —É—Å–ø–µ—à–µ–Ω."""
    mock_glm.analyze_multiple_images.return_value = ProviderResponse(
        success=True,
        content="–û–ø–∏—Å–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞",
        provider_name="glm"
    )
    
    chain = VisionProviderChain([mock_glm, mock_gemini])
    result = await chain.analyze_image(b"fake_image")
    
    assert result.success
    assert result.provider_name == "glm"
    mock_gemini.analyze_multiple_images.assert_not_called()


@pytest.mark.asyncio
async def test_vision_chain_fallback_on_failure(mock_glm, mock_gemini):
    """–¢–µ—Å—Ç: fallback –Ω–∞ –≤—Ç–æ—Ä–æ–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä –ø—Ä–∏ –æ—à–∏–±–∫–µ –ø–µ—Ä–≤–æ–≥–æ."""
    mock_glm.analyze_multiple_images.return_value = ProviderResponse(
        success=False,
        content="",
        provider_name="glm",
        error_message="API Error"
    )
    mock_gemini.analyze_multiple_images.return_value = ProviderResponse(
        success=True,
        content="–û–ø–∏—Å–∞–Ω–∏–µ –æ—Ç Gemini",
        provider_name="gemini"
    )
    
    chain = VisionProviderChain([mock_glm, mock_gemini])
    result = await chain.analyze_image(b"fake_image")
    
    assert result.success
    assert result.provider_name == "gemini"


@pytest.mark.asyncio
async def test_vision_chain_all_failed(mock_glm, mock_gemini):
    """–¢–µ—Å—Ç: –≤—Å–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã."""
    mock_glm.analyze_multiple_images.return_value = ProviderResponse(
        success=False, content="", provider_name="glm", error_message="Error 1"
    )
    mock_gemini.analyze_multiple_images.return_value = ProviderResponse(
        success=False, content="", provider_name="gemini", error_message="Error 2"
    )
    
    chain = VisionProviderChain([mock_glm, mock_gemini])
    
    with pytest.raises(RuntimeError) as exc_info:
        await chain.analyze_image(b"fake_image")
    
    assert "–í—Å–µ Vision –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã" in str(exc_info.value)
```

---

## üìã CHECKLIST –ò–ù–¢–ï–ì–†–ê–¶–ò–ò

- [ ] –ü–æ–ª—É—á–∏—Ç—å API –∫–ª—é—á Z.AI: https://z.ai/manage-apikey/apikey-list
- [ ] –ü–æ–ª—É—á–∏—Ç—å API –∫–ª—é—á Gemini: https://aistudio.google.com/apikey
- [ ] –î–æ–±–∞–≤–∏—Ç—å –∫–ª—é—á–∏ –≤ `.env`
- [ ] –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å GLM Vision (–∞–Ω–∞–ª–∏–∑ —Ñ–æ—Ç–æ)
- [ ] –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å GLM Text (–≥–µ–Ω–µ—Ä–∞—Ü–∏—è)
- [ ] –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å Gemini –∫–∞–∫ fallback
- [ ] –£–±–µ–¥–∏—Ç—å—Å—è —á—Ç–æ chain —Ä–∞–±–æ—Ç–∞–µ—Ç –ø—Ä–∏ –æ—Ç–∫–ª—é—á–µ–Ω–∏–∏ GLM

---

*–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è AI –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ v1.0*
